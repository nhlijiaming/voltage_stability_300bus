{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "import scipy.io as sio\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print(time.strftime('%H:%M:%S  ',time.localtime(time.time())), end=\"\")\n",
    "print(\"Loading data\")\n",
    "\n",
    "#####   Loading multiple data files with sio    ######\n",
    "\n",
    "matfile1 = sio.loadmat('data_case300_1440K/data_case300_feasible_530_train.mat')\n",
    "matfile2 = sio.loadmat('data_case300_1440K/data_case300_feasible_530_test.mat')\n",
    "feasible_data = 0.95*np.concatenate([matfile1['feasible_data'], matfile2['feasible_data']], axis=1)\n",
    "feasible_labels = -1 * np.ones(feasible_data.shape[1], dtype=np.int8)\n",
    "del matfile1\n",
    "del matfile2\n",
    "\n",
    "matfile1 = sio.loadmat('data_case300_1440K/data_case300_infeasible_530_train.mat')\n",
    "matfile2 = sio.loadmat('data_case300_1440K/data_case300_infeasible_530_test.mat')\n",
    "infeasible_data = 1.05*np.concatenate([matfile1['infeasible_data'], matfile2['infeasible_data']], axis=1)\n",
    "infeasible_labels = np.ones(infeasible_data.shape[1], dtype=np.int8)\n",
    "del matfile1\n",
    "del matfile2\n",
    "\n",
    "data = np.concatenate([feasible_data, infeasible_data], axis=1).T\n",
    "labels = np.concatenate([feasible_labels, infeasible_labels]).reshape(-1, 1)\n",
    "del feasible_data \n",
    "del infeasible_data\n",
    "del feasible_labels\n",
    "del infeasible_labels\n",
    "\n",
    "print(data.shape, labels.shape)\n",
    "\n",
    "train_n = 1000000 # number of training samples\n",
    "total_n = 1440000 # number of samples in total\n",
    "train_n_c = train_n // 2\n",
    "total_n_c = total_n // 2\n",
    "\n",
    "train_i = np.array([np.arange(0, train_n_c),np.arange(0, train_n_c) + total_n_c]).T.reshape(-1)\n",
    "val_i = np.array([np.arange(train_n_c, total_n_c), np.arange(train_n_c, total_n_c)+  total_n_c]).T.reshape(-1)\n",
    "\n",
    "train_data = data[train_i, :]\n",
    "train_labels = labels[train_i, :]\n",
    "val_data = data[val_i, :]\n",
    "val_labels = labels[val_i, :]\n",
    "del data\n",
    "del labels\n",
    "\n",
    "mean_x = 0\n",
    "std_x = 1\n",
    "\n",
    "mean_x = np.mean(train_data, axis = 0)\n",
    "std_x = np.std(train_data, axis = 0)\n",
    "std_x[np.less(std_x, 0.001)] = 0.001\n",
    "\n",
    "\n",
    "print(time.strftime('%H:%M:%S  ',time.localtime(time.time())), end=\"\")\n",
    "print(\"Finished data\")\n",
    "\n",
    "seed_difference = 0\n",
    "if seed_difference != 0:\n",
    "    print(f'***************************')\n",
    "    print(f'***** SEED DIFF: {seed_difference} ********')\n",
    "    print(f'***************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "# Train a base model on the binary data set\n",
    "\n",
    "datafile_prefix = 'data_case300_1440K/'\n",
    "\n",
    "hyper_param_template={'traindatasetSize' : 1000000, 'neurons' : [1024, 1],\n",
    "'learningRate1' : 0.01, 'learningRate2' : 0.01, 'momentum' : 0.9,\n",
    "'trainBatchSize' : 5000, 'trainingEpochs' : 2000, 'learningRateChangeAfter' : 1.0,\n",
    "'valBatchSize' : 1000, 'displayPerEpochs' : 50, 'valPerEpochs' : 50,\n",
    "'savePerEpochs' : 99999500, 'Regularizer' : 'L2', 'lambda' : 0.0, 'dropout_keepprob' : 1.0}\n",
    "\n",
    "hyper_param_list = [hyper_param_template.copy() for i in range(1)]\n",
    "\n",
    "prefixPath = os.path.join(os.getcwd(), 'models') + '/'\n",
    "\n",
    "loadFilename = None\n",
    "\n",
    "def bn_layer(x, scope, is_training, epsilon=0.001, decay=0.99, reuse=None):\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        shape = x.get_shape().as_list()\n",
    "        # gamma: a trainable scale factor\n",
    "        gamma = tf.get_variable(\"gamma\", shape[-1], initializer=tf.constant_initializer(1.0), trainable=True)\n",
    "        # beta: a trainable shift value\n",
    "        beta = tf.get_variable(\"beta\", shape[-1], initializer=tf.constant_initializer(0.0), trainable=True)\n",
    "        moving_avg = tf.get_variable(\"moving_avg\", shape[-1], initializer=tf.constant_initializer(0.0), trainable=False)\n",
    "        moving_var = tf.get_variable(\"moving_var\", shape[-1], initializer=tf.constant_initializer(1.0), trainable=False)\n",
    "        if is_training:\n",
    "            # tf.nn.moments == Calculate the mean and the variance of the tensor x\n",
    "            avg, var = tf.nn.moments(x, np.arange(len(shape)-1), keep_dims=True)\n",
    "            avg=tf.reshape(avg, [avg.shape.as_list()[-1]])\n",
    "            var=tf.reshape(var, [var.shape.as_list()[-1]])\n",
    "            #update_moving_avg = moving_averages.assign_moving_average(moving_avg, avg, decay)\n",
    "            update_moving_avg=tf.assign(moving_avg, moving_avg*decay+avg*(1-decay))\n",
    "            #update_moving_var = moving_averages.assign_moving_average(moving_var, var, decay)\n",
    "            update_moving_var=tf.assign(moving_var, moving_var*decay+var*(1-decay))\n",
    "            control_inputs = [update_moving_avg, update_moving_var]\n",
    "            with tf.control_dependencies(control_inputs):\n",
    "                output = tf.nn.batch_normalization(x, avg, var, offset=beta, scale=gamma, variance_epsilon=epsilon)\n",
    "        else:\n",
    "            avg = moving_avg\n",
    "            var = moving_var\n",
    "            output = tf.nn.batch_normalization(x, avg, var, offset=beta, scale=gamma, variance_epsilon=epsilon)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def bn_layer_top(x, scope, is_training, epsilon=0.001, decay=0.99):\n",
    "    if is_training:\n",
    "        return bn_layer(x=x, scope=scope, epsilon=epsilon, decay=decay, is_training=True, reuse=None)\n",
    "    else:\n",
    "        return bn_layer(x=x, scope=scope, epsilon=epsilon, decay=decay, is_training=False, reuse=True)\n",
    "\n",
    "def getScores(sess, layer, data):\n",
    "    dataSize = data.shape[0]\n",
    "    print(dataSize)\n",
    "    scores = []\n",
    "    for i in range(0, dataSize):\n",
    "        x = [data[i]]\n",
    "        p = sess.run(layer, feed_dict={X_test:x, dropout_keepprob:1.0})\n",
    "        scores.append(p)\n",
    "    scores = np.array(scores).reshape(dataSize, -1)\n",
    "    return scores\n",
    "\n",
    "def validation(sess, val_x_data, val_y_data, hyper_param, getLoss=False, computeMean=True):\n",
    "    valBatchSize = hyper_param['valBatchSize']\n",
    "    valDataSize = val_x_data.shape[0]\n",
    "    accu = np.zeros(valDataSize//valBatchSize)\n",
    "    loss = np.zeros(valDataSize//valBatchSize)\n",
    "    for i in range(0, valDataSize, valBatchSize):\n",
    "        val_x = val_x_data[i:i+valBatchSize]\n",
    "        val_y = val_y_data[i:i+valBatchSize]\n",
    "        accu[i//valBatchSize], loss[i//valBatchSize] = sess.run([accuracy, cost_val], feed_dict={X_val:val_x, Y_val:val_y, dropout_keepprob:1.0})\n",
    "\n",
    "    if computeMean is True:\n",
    "        if getLoss is True:\n",
    "            return accu.mean(), loss.mean()\n",
    "        else:\n",
    "            return accu.mean()\n",
    "    elif getLoss is True:\n",
    "        return accu, loss\n",
    "    else:\n",
    "        return accu\n",
    "\n",
    "def train(sess, train_op, train_x_data, train_y_data,  val_x_data, val_y_data, hyper_param):\n",
    "    global itersInTotal, totalTrainingIters, global_trainRecord, global_valRecord, global_testRecord\n",
    "    \n",
    "    datasetSize = hyper_param['traindatasetSize']\n",
    "    learningRate1 = hyper_param['learningRate1']\n",
    "    learningRate2 = hyper_param['learningRate2']\n",
    "    trainBatchSize = hyper_param['trainBatchSize']\n",
    "    trainingEpochs = hyper_param['trainingEpochs']\n",
    "    trainingIters = hyper_param['traindatasetSize'] * trainingEpochs / trainBatchSize\n",
    "    learningRateChangeAfter = int(hyper_param['learningRateChangeAfter'] * trainingIters)\n",
    "    displayPerIters = hyper_param['displayPerEpochs'] * datasetSize / trainBatchSize\n",
    "    valPerIters = hyper_param['valPerEpochs'] * datasetSize / trainBatchSize\n",
    "    savePerIters = hyper_param['savePerEpochs'] * datasetSize / trainBatchSize\n",
    "    neurons = hyper_param['neurons']\n",
    "    keepprob = hyper_param['dropout_keepprob']\n",
    "\n",
    "    iters = 0 * train_x_data.shape[0] / trainBatchSize\n",
    "    trainDataSize = train_x_data.shape[0]\n",
    "    learningRate = learningRate1\n",
    "#     modelFilename = prefixPath + 'model_1L_' + str(neurons[0]) + 'Ns_BZ' + str(trainBatchSize) +'_SZ%dK_%depochs_reduced.ckpt'\n",
    "\n",
    "#    pkl = open('lossRecord_1L_512Ns_BZ200_SZ120K.pkl','rb')\n",
    "#    global_trainRecord = pickle.load(pkl)\n",
    "#    global_valRecord = pickle.load(pkl)\n",
    "#    pkl.close()\n",
    "    \n",
    "    f0 = plt.figure(0, figsize=(12,6))\n",
    "    ax0 = f0.gca()\n",
    "    ax0.plot([0])\n",
    "    f0.canvas.draw()\n",
    "\n",
    "    print(time.strftime('%H:%M:%S  ',time.localtime(time.time())), end=\"\")\n",
    "    print(\"Start training\")\n",
    "    \n",
    "    for epoch in range(trainingEpochs):\n",
    "        for i in range(0, trainDataSize, trainBatchSize):\n",
    "            if iters > trainingIters:\n",
    "                break\n",
    "            iters = iters + 1\n",
    "            itersInTotal = itersInTotal + 1\n",
    "            train_x = train_x_data[i:i+trainBatchSize]\n",
    "            train_y = train_y_data[i:i+trainBatchSize]\n",
    "            _, c = sess.run([train_op, cost], feed_dict={X:train_x, Y:train_y, learning_rate:learningRate, dropout_keepprob:keepprob})\n",
    "            if iters % displayPerIters == 0:\n",
    "                ETA = (time.time() - beginTime) / (itersInTotal / totalTrainingIters) * (1 - itersInTotal / totalTrainingIters) / 60\n",
    "                print(time.strftime('%H:%M:%S',time.localtime(time.time())), 'Est %.1fmins Elaps %.1fmins '%(ETA,(time.time() - beginTime) / 60), end=\"\")\n",
    "                print(\"Iters %d training cost: %f.\" % (iters, c))\n",
    "                \n",
    "            if iters % valPerIters == 0:\n",
    "                print(time.strftime('%H:%M:%S  ',time.localtime(time.time())),end=\"\")\n",
    "                ret = validation(sess, train_x_data, train_y_data, hyper_param, getLoss=True)\n",
    "                global_trainRecord.append([iters, ret[0], ret[1]])\n",
    "                ret = validation(sess, val_x_data, val_y_data, hyper_param, getLoss=True)\n",
    "                print(\"   Iters %d (epochs %.1f) validation loss: %f accuracy: %f\" % (iters, iters*trainBatchSize/trainDataSize, ret[1], ret[0]))\n",
    "                global_valRecord.append([iters, ret[0], ret[1]])\n",
    "                \n",
    "                ax0.cla()\n",
    "                ax0.plot(np.array(global_trainRecord)[:,0], np.array(global_trainRecord)[:,2], 'r', label=\"Train\")\n",
    "                ax0.plot(np.array(global_valRecord)[:,0], np.array(global_valRecord)[:,2], 'b', label=\"Val\")\n",
    "                ax0.legend()\n",
    "                ax0.grid(True)\n",
    "                f0.canvas.draw()\n",
    "    return\n",
    "\n",
    "\n",
    "beginTime = time.time()\n",
    "totalTrainingIters = hyper_param_list[-1]['traindatasetSize'] * hyper_param_list[-1]['trainingEpochs'] / hyper_param_list[-1]['trainBatchSize'] * len(hyper_param_list)\n",
    "itersInTotal = 0\n",
    "inputdimension = train_data.shape[1]\n",
    "\n",
    "trainingAccuracy = np.zeros(len(hyper_param_list))\n",
    "testingAccuracy = np.zeros(len(hyper_param_list))\n",
    "trainingLoss = np.zeros(len(hyper_param_list))\n",
    "testingLoss = np.zeros(len(hyper_param_list))\n",
    "\n",
    "\n",
    "for hyper_param in hyper_param_list:\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(20200224+seed_difference)\n",
    "\n",
    "    datasetSize = hyper_param['traindatasetSize']\n",
    "    trainBatchSize = hyper_param['trainBatchSize']\n",
    "    valBatchSize = hyper_param['valBatchSize']\n",
    "    trainingEpochs = hyper_param['trainingEpochs']\n",
    "    neurons = hyper_param['neurons']\n",
    "    momentum = hyper_param['momentum']\n",
    "    Regularizer = hyper_param['Regularizer']\n",
    "    lamb = hyper_param['lambda']\n",
    "\n",
    "\n",
    "    train_x_data = train_data[:datasetSize, :]\n",
    "    train_y_data = train_labels[:datasetSize, :]\n",
    "    val_x_data = val_data[:, :]\n",
    "    val_y_data = val_labels[:, :]\n",
    "\n",
    "    print('Train set size:', len(train_x_data), '  Val set size:', len(val_x_data))\n",
    "\n",
    "    mean_x = 0\n",
    "    std_x = 1\n",
    "\n",
    "    mean_x = np.mean(train_x_data, axis = 0)\n",
    "    std_x = np.std(train_x_data, axis = 0)\n",
    "    std_x[np.less(std_x, 0.001)] = 0.001\n",
    "\n",
    "    train_x_data = (train_x_data - mean_x) / std_x\n",
    "    val_x_data = (val_x_data - mean_x) / std_x\n",
    "#     test_x_data = (test_data - mean_x) / std_x\n",
    "#    test_x_data = (test_x_data - mean_x) / std_x\n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape=(trainBatchSize, inputdimension), name='X')\n",
    "    Y = tf.placeholder(tf.float32, shape=(trainBatchSize, neurons[-1]), name='Y')\n",
    "    X_val = tf.placeholder(tf.float32, shape=(valBatchSize, inputdimension), name='X_val')\n",
    "    Y_val = tf.placeholder(tf.float32, shape=(valBatchSize, neurons[-1]), name='Y_val')\n",
    "    X_test = tf.placeholder(tf.float32, shape=(1, inputdimension), name='X_test')\n",
    "\n",
    "    weights = {}\n",
    "    biases = {}\n",
    "    neuronsofLastLayer = inputdimension\n",
    "    for i, n in enumerate(neurons):\n",
    "        key = 'fc' + str(i+1)\n",
    "        if (i==1):\n",
    "            key='fc2_cla'\n",
    "        print(key)\n",
    "        weights.update({key: tf.Variable(tf.truncated_normal([neuronsofLastLayer, neurons[i]], stddev=np.sqrt(2./neuronsofLastLayer)), name='weights_'+key)})\n",
    "        biases.update( {key: tf.Variable(tf.truncated_normal([neurons[i]], stddev=0.01), name='biases_'+key)})\n",
    "        neuronsofLastLayer = neurons[i]\n",
    "\n",
    "    dropout_keepprob = tf.placeholder(tf.float32, shape=[])\n",
    "    def postprocessing(layer, scope='Layer', is_training=False):\n",
    "#         with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "        t = tf.nn.relu(layer)\n",
    "#         t = bn_layer_top(t, scope, is_training)\n",
    "        # t = tf.nn.dropout(t, dropout_keepprob)\n",
    "        # t = tf.layers.batch_normalization(inputs=t, name = name+'_bn', training=training)\n",
    "        return t\n",
    "\n",
    "    layers = []\n",
    "    layers_val = []\n",
    "    layers_test = []\n",
    "    if len(neurons) == 1:\n",
    "        outputLayer = tf.matmul(X, weights[key]) + biases[key]\n",
    "        outputLayer_val = tf.matmul(X_val, weights[key]) + biases[key]\n",
    "    else:\n",
    "        for i, n in enumerate(neurons):\n",
    "            key = 'fc' + str(i+1)\n",
    "            if (i == 1):\n",
    "                key = 'fc2_cla'\n",
    "            if i == 0:\n",
    "                layers.append(postprocessing(tf.matmul(X, weights[key]) + biases[key], scope=key, is_training=True))\n",
    "                layers_val.append(postprocessing(tf.matmul(X_val, weights[key]) + biases[key], scope=key))\n",
    "                layers_test.append(postprocessing(tf.matmul(X_test, weights[key]) + biases[key], scope=key))\n",
    "            elif i == len(neurons)-1:\n",
    "                outputLayer = tf.matmul(layers[-1], weights[key]) + biases[key]\n",
    "                outputLayer_val = tf.matmul(layers_val[-1], weights[key]) + biases[key]\n",
    "                outputLayer_test = tf.matmul(layers_test[-1], weights[key]) + biases[key]\n",
    "            else:\n",
    "                layers.append(postprocessing(tf.add(tf.matmul(layers[-1], weights[key]), biases[key]), scope=key, is_training=True))\n",
    "                layers_val.append(postprocessing(tf.add(tf.matmul(layers_val[-1], weights[key]), biases[key]), scope=key))\n",
    "                layers_test.append(postprocessing(tf.add(tf.matmul(layers_test[-1], weights[key]), biases[key], scope=key)))\n",
    "\n",
    "    def hinge_loss(pred, groundtruth):\n",
    "        return tf.reduce_mean(tf.maximum(0., 1. - pred * groundtruth))\n",
    "\n",
    "    def regularizer(W):\n",
    "        if Regularizer == 'L1':\n",
    "            return lamb * tf.reduce_mean(tf.abs(W))\n",
    "        elif Regularizer == 'L2':\n",
    "            return lamb * tf.reduce_mean(tf.square(W))\n",
    "\n",
    "    cost = hinge_loss(outputLayer, Y)# + regularizer(weights['fc2_cla']) + regularizer(weights['fc1'])\n",
    "    cost_val = hinge_loss(outputLayer_val, Y_val)\n",
    "\n",
    "    correct_pred = tf.equal(tf.greater(outputLayer_val, 0), tf.greater(Y_val, 0))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "\n",
    "#     with tf.variable_scope(\"fc1\", reuse=True):\n",
    "#         fc1_beta = tf.get_variable(\"beta\")\n",
    "#         fc1_gamma = tf.get_variable(\"gamma\")\n",
    "    \n",
    "    var1 = [weights['fc1'], biases['fc1'], weights['fc2_cla'], biases['fc2_cla']] #, fc1_beta, fc1_gamma]\n",
    "    # var2 = [weights['fc2_cla'], biases['fc2_cla']]\n",
    "    # train_op1 = tf.train.MomentumOptimizer(learning_rate, momentum, use_nesterov=True).minimize(cost, var_list=var1)\n",
    "    # train_op2 = tf.train.MomentumOptimizer(learning_rate, momentum, use_nesterov=True).minimize(cost, var_list=var2)\n",
    "    # train_op = tf.group(train_op1, train_op2)\n",
    "#     train_op = tf.train.MomentumOptimizer(learning_rate, momentum, use_nesterov=True).minimize(cost, var_list=var1)\n",
    "\n",
    "    # train_op = tf.train.MomentumOptimizer(learning_rate, momentum, use_nesterov=True).minimize(cost)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost, var_list=var1)\n",
    "\n",
    "    sess = tf.Session()\n",
    "    saver = tf.train.Saver(var1)\n",
    "    # saver = tf.train.Saver()\n",
    "\n",
    "    if loadFilename is not None:\n",
    "#         saver.restore(sess, prefixPath+loadFilename)\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver.restore(sess, 'saved_models/1024_530')\n",
    "        print(time.strftime('%H:%M:%S  ',time.localtime(time.time())),end=\"\")\n",
    "        print('Pre-trained model loaded.')\n",
    "    else:\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "    print('Accuracy before training: ', validation(sess, val_x_data, val_y_data, hyper_param, getLoss=True))\n",
    "\n",
    "    global_trainRecord = []\n",
    "    global_valRecord = []\n",
    "    global_testRecord = []\n",
    "    train_cost_history = train(sess, train_op, train_x_data, train_y_data, val_x_data, val_y_data, hyper_param)\n",
    "\n",
    "    idx = hyper_param_list.index(hyper_param)\n",
    "    trainingAccuracy[idx], trainingLoss[idx] = validation(sess, train_x_data, train_y_data, hyper_param, getLoss=True)\n",
    "    testingAccuracy[idx], testingLoss[idx] = validation(sess, val_x_data, val_y_data, hyper_param, getLoss=True)\n",
    "\n",
    "    print('network:', neurons, 'dataset:', datafile_prefix, \"size:\", datasetSize)\n",
    "    print(\"train accuracy %.6f, validation accuracy %.6f\"%(trainingAccuracy[idx], testingAccuracy[idx]))\n",
    "    print(\"train loss %.6f, validation loss %.6f\"%(trainingLoss[idx], testingLoss[idx]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save base model\n",
    "saver.save(sess, 'saved_models/1024_530_current')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load margin data\n",
    "\n",
    "matfile = sio.loadmat('data_case300_11K_margin/data_case300_margin_ready_530_train.mat')\n",
    "margin_data = matfile['margin_data'].T\n",
    "boundary_data = matfile['boundary_data'].T\n",
    "matfile = sio.loadmat('data_case300_11K_margin/data_case300_distance_train.mat')\n",
    "dis2boundary = matfile['distance'].T\n",
    "margin_data = (margin_data - mean_x) / std_x\n",
    "boundary_data = (boundary_data - mean_x) / std_x\n",
    "delta_data = boundary_data - margin_data\n",
    "\n",
    "matfile = sio.loadmat('data_case300_11K_margin/data_case300_margin_ready_530_val.mat')\n",
    "reg_val_x_data = matfile['margin_data'].T[:1400]\n",
    "reg_val_x_data =  (reg_val_x_data - mean_x) / std_x\n",
    "matfile = sio.loadmat('data_case300_11K_margin/data_case300_distance_val.mat')\n",
    "reg_val_y_data = matfile['distance'].T[:1400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a transfer learning model on the margin data set\n",
    "\n",
    "def bn_layer(x, scope, is_training, epsilon=0.001, decay=0.99, reuse=None):\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        shape = x.get_shape().as_list()\n",
    "        # gamma: a trainable scale factor\n",
    "        gamma = tf.get_variable(\"gamma\", shape[-1], initializer=tf.constant_initializer(1.0), trainable=True)\n",
    "        # beta: a trainable shift value\n",
    "        beta = tf.get_variable(\"beta\", shape[-1], initializer=tf.constant_initializer(0.0), trainable=True)\n",
    "        moving_avg = tf.get_variable(\"moving_avg\", shape[-1], initializer=tf.constant_initializer(0.0), trainable=False)\n",
    "        moving_var = tf.get_variable(\"moving_var\", shape[-1], initializer=tf.constant_initializer(1.0), trainable=False)\n",
    "        if is_training:\n",
    "            # tf.nn.moments == Calculate the mean and the variance of the tensor x\n",
    "            avg, var = tf.nn.moments(x, np.arange(len(shape)-1), keep_dims=True)\n",
    "            avg=tf.reshape(avg, [avg.shape.as_list()[-1]])\n",
    "            var=tf.reshape(var, [var.shape.as_list()[-1]])\n",
    "            #update_moving_avg = moving_averages.assign_moving_average(moving_avg, avg, decay)\n",
    "            update_moving_avg=tf.assign(moving_avg, moving_avg*decay+avg*(1-decay))\n",
    "            #update_moving_var = moving_averages.assign_moving_average(moving_var, var, decay)\n",
    "            update_moving_var=tf.assign(moving_var, moving_var*decay+var*(1-decay))\n",
    "            control_inputs = [update_moving_avg, update_moving_var]\n",
    "            with tf.control_dependencies(control_inputs):\n",
    "                output = tf.nn.batch_normalization(x, avg, var, offset=beta, scale=gamma, variance_epsilon=epsilon)\n",
    "        else:\n",
    "            avg = moving_avg\n",
    "            var = moving_var\n",
    "            output = tf.nn.batch_normalization(x, avg, var, offset=beta, scale=gamma, variance_epsilon=epsilon)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def bn_layer_top(x, scope, is_training, epsilon=0.001, decay=0.99):\n",
    "    if is_training:\n",
    "        return bn_layer(x=x, scope=scope, epsilon=epsilon, decay=decay, is_training=True, reuse=None)\n",
    "    else:\n",
    "        return bn_layer(x=x, scope=scope, epsilon=epsilon, decay=decay, is_training=False, reuse=True)\n",
    "\n",
    "def reg_validation(sess, val_x_data, val_y_data, hyper_param):\n",
    "    valBatchSize = hyper_param['valBatchSize']\n",
    "    valDataSize = val_x_data.shape[0]\n",
    "    loss = np.zeros(valDataSize//valBatchSize)\n",
    "    for i in range(0, valDataSize, valBatchSize):\n",
    "        val_x = val_x_data[i:i+valBatchSize]\n",
    "        val_y = val_y_data[i:i+valBatchSize]\n",
    "        loss[i//valBatchSize],out = sess.run([reg_cost_val, reg_layer_val], feed_dict={X_val:val_x, reg_Y_val:val_y})\n",
    "#     plt.plot(out,'.')\n",
    "#     plt.plot(val_y,'.')\n",
    "#     plt.show()\n",
    "    return loss.mean()\n",
    "\n",
    "def reg_train(sess, reg_train_op, reg_cost, train_x_data, train_y_data, train_delta_data, val_x_data, val_y_data, hyper_param):\n",
    "    global itersInTotal, totalTrainingIters, global_trainRecord, global_valRecord, global_testRecord\n",
    "    np.random.seed(20200224+seed_difference)\n",
    "    \n",
    "    datasetSize = hyper_param['traindatasetSize']\n",
    "    learningRate1 = hyper_param['learningRate1']\n",
    "    learningRate2 = hyper_param['learningRate2']\n",
    "    trainBatchSize = hyper_param['trainBatchSize']\n",
    "    trainingEpochs = hyper_param['trainingEpochs']\n",
    "    trainingIters = hyper_param['traindatasetSize'] * trainingEpochs / trainBatchSize\n",
    "    print(trainingIters)\n",
    "    learningRateChangeAfter = int(hyper_param['learningRateChangeAfter'] * trainingIters)\n",
    "    displayPerIters = hyper_param['displayPerEpochs'] * datasetSize / trainBatchSize\n",
    "    valPerIters = hyper_param['valPerEpochs'] * datasetSize / trainBatchSize\n",
    "    savePerIters = hyper_param['savePerEpochs'] * datasetSize / trainBatchSize\n",
    "    neurons = hyper_param['neurons']\n",
    "    keepprob = hyper_param['dropout_keepprob']\n",
    "    DA_flag = reg_hyper_param['dataAugmentation']\n",
    "\n",
    "    iters = itersInTotal\n",
    "    trainDataSize = train_x_data.shape[0]\n",
    "    learningRate = learningRate1\n",
    "    \n",
    "    for epoch in range(trainingEpochs):\n",
    "        if not DA_flag:# or epoch<5000:\n",
    "            np.random.seed(20200224+seed_difference)\n",
    "#         learningRate1 = learningRate1 * 0.99995\n",
    "#         learningRate = learningRate1\n",
    "        for i in range(0, trainDataSize, trainBatchSize):\n",
    "            iters = iters + 1\n",
    "            itersInTotal = itersInTotal + 1\n",
    "#             if (iters > learningRateChangeAfter and learningRate != learningRate2) or (iters == learningRateChangeAfter):\n",
    "#                 learningRate = learningRate2\n",
    "#                 print(time.strftime('%H:%M:%S  ',time.localtime(time.time())),end=\"\")\n",
    "#                 print('Now learning rate changes to %f' % learningRate)\n",
    "            factor = np.random.uniform(low=0.03, high=1.0, size=(trainBatchSize, 1))\n",
    "            train_x = train_x_data[i:i+trainBatchSize] + factor * train_delta_data[i:i+trainBatchSize, :]\n",
    "            train_y = (1-factor) * train_y_data[i:i+trainBatchSize]\n",
    "            _, c = sess.run([reg_train_op, reg_cost], feed_dict={X:train_x, reg_Y:train_y, learning_rate:learningRate})\n",
    "            if iters % displayPerIters == 0:\n",
    "                ETA = (time.time() - beginTime) / (itersInTotal / totalTrainingIters) * (1 - itersInTotal / totalTrainingIters) / 60\n",
    "                print(time.strftime('%H:%M:%S',time.localtime(time.time())), 'Est %.1fmins Elaps %.1fmins '%(ETA,(time.time() - beginTime) / 60), end=\"\")\n",
    "                print(\"Iters %d training MSE: %.6f.\" % (iters, c))\n",
    "                \n",
    "            if iters % valPerIters == 0:\n",
    "                global_trainRecord.append([iters, c])\n",
    "                ret = reg_validation(sess, val_x_data, val_y_data, hyper_param)\n",
    "                print(\"           Iters %d (epochs %.1f) validation MSE: %.6f\" % (iters, iters*trainBatchSize/trainDataSize, ret))\n",
    "                global_valRecord.append([iters, ret])\n",
    "                \n",
    "    return\n",
    "\n",
    "reg_hyper_param={'traindatasetSize' : 10000, 'neurons' : [1024, 1],\n",
    "'learningRate1' : 0.001, 'learningRate2' : 0.00001, 'momentum' : 0.9,\n",
    "'trainBatchSize' : 5000, 'trainingEpochs' : 89200, 'learningRateChangeAfter' : 1.4,\n",
    "'valBatchSize' : 200, 'displayPerEpochs' : 200, 'valPerEpochs' : 200,\n",
    "'savePerEpochs' : 99999500, 'Regularizer' : 'L2', 'lambda' : 0.0000, 'dropout_keepprob' : 1.0, 'dataAugmentation': True}\n",
    "\n",
    "datasetSize = reg_hyper_param['traindatasetSize']\n",
    "reg_train_x_data = margin_data[:datasetSize, :]\n",
    "reg_train_y_data = dis2boundary[:datasetSize]\n",
    "reg_train_delta_data = delta_data[:datasetSize, :]\n",
    "model_type = ''\n",
    "DA_flag = reg_hyper_param['dataAugmentation']\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "inputdimension = reg_train_x_data.shape[1]\n",
    "datasetSize = reg_hyper_param['traindatasetSize']\n",
    "trainBatchSize = reg_hyper_param['trainBatchSize']\n",
    "valBatchSize = reg_hyper_param['valBatchSize']\n",
    "trainingEpochs = reg_hyper_param['trainingEpochs']\n",
    "neurons = reg_hyper_param['neurons']\n",
    "# momentum = reg_hyper_param['momentum']\n",
    "# Regularizer = reg_hyper_param['Regularizer']\n",
    "lamb = reg_hyper_param['lambda']\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(trainBatchSize, inputdimension), name='X')\n",
    "Y = tf.placeholder(tf.float32, shape=(trainBatchSize, neurons[-1]), name='Y')\n",
    "X_val = tf.placeholder(tf.float32, shape=(valBatchSize, inputdimension), name='X_val')\n",
    "Y_val = tf.placeholder(tf.float32, shape=(valBatchSize, neurons[-1]), name='Y_val')\n",
    "X_test = tf.placeholder(tf.float32, shape=(1, inputdimension), name='X_test')\n",
    "\n",
    "weights = {}\n",
    "biases = {}\n",
    "neuronsofLastLayer = inputdimension\n",
    "for i, n in enumerate(neurons):\n",
    "    key = 'fc' + str(i+1)\n",
    "    if (i==1):\n",
    "        key='fc2_cla'\n",
    "    weights.update({key: tf.Variable(tf.truncated_normal([neuronsofLastLayer, neurons[i]], stddev=np.sqrt(2./neuronsofLastLayer), seed=20200401+seed_difference), name='weights_'+key)})\n",
    "    biases.update( {key: tf.Variable(tf.truncated_normal([neurons[i]], stddev=0.0001, seed=20200401+seed_difference), name='biases_'+key)})\n",
    "    neuronsofLastLayer = neurons[i]\n",
    "\n",
    "# dropout_keepprob = tf.placeholder(tf.float32, shape=[])\n",
    "def postprocessing(layer, scope='Layer', is_training=False):\n",
    "#     layer = bn_layer_top(layer, scope, is_training)\n",
    "    layer = tf.nn.relu(layer)\n",
    "    return layer\n",
    "\n",
    "layers = []\n",
    "layers_val = []\n",
    "layers_test = []\n",
    "if len(neurons) == 1:\n",
    "    outputLayer = tf.matmul(X, weights[key]) + biases[key]\n",
    "    outputLayer_val = tf.matmul(X_val, weights[key]) + biases[key]\n",
    "else:\n",
    "    for i, n in enumerate(neurons):\n",
    "        key = 'fc' + str(i+1)\n",
    "        if (i == 1):\n",
    "            key = 'fc2_cla'\n",
    "        if i == 0:\n",
    "            layers.append(postprocessing(tf.matmul(X, weights[key]) + biases[key], scope=key, is_training=True))\n",
    "            layers_val.append(postprocessing(tf.matmul(X_val, weights[key]) + biases[key], scope=key))\n",
    "            layers_test.append(postprocessing(tf.matmul(X_test, weights[key]) + biases[key], scope=key))\n",
    "        elif i == len(neurons)-1:\n",
    "            outputLayer = tf.matmul(layers[-1], weights[key]) + biases[key]\n",
    "            outputLayer_val = tf.matmul(layers_val[-1], weights[key]) + biases[key]\n",
    "            outputLayer_test = tf.matmul(layers_test[-1], weights[key]) + biases[key]\n",
    "        else:\n",
    "            layers.append(postprocessing(tf.add(tf.matmul(layers[-1], weights[key]), biases[key]), scope=key, is_training=True))\n",
    "            layers_val.append(postprocessing(tf.add(tf.matmul(layers_val[-1], weights[key]), biases[key]), scope=key))\n",
    "            layers_test.append(postprocessing(tf.add(tf.matmul(layers_test[-1], weights[key]), biases[key], scope=key)))\n",
    "\n",
    "######### First Layer of regressor\n",
    "reg_weights1 = tf.Variable(tf.truncated_normal([1024,128], stddev=0.00005, seed=20200224+seed_difference), name='weights_reg')\n",
    "reg_biases1 = tf.Variable(tf.truncated_normal([128], stddev=0.00005, seed=20200224+seed_difference), name='biases_reg')\n",
    "# reg_weights1 = tf.Variable(tf.truncated_normal([256,64], stddev=np.sqrt(2./256), seed=20200224), name='weights1_reg')\n",
    "# reg_biases1 = tf.Variable(tf.truncated_normal([64], stddev=0.0001, seed=20200224), name='biases1_reg')\n",
    "\n",
    "reg_layer1 = tf.matmul(layers[-1], reg_weights1) + reg_biases1\n",
    "reg_layer1 = tf.nn.relu(reg_layer1); model_type += 'ReLU'\n",
    "model_type += ' then '\n",
    "reg_layer1 = bn_layer_top(reg_layer1, 'reg1', True); model_type += 'BN2'\n",
    "\n",
    "reg_layer1_val = tf.matmul(layers_val[-1], reg_weights1) + reg_biases1\n",
    "if 'ReLU' in model_type:\n",
    "    reg_layer1_val = tf.nn.relu(reg_layer1_val)\n",
    "if 'BN2' in model_type:\n",
    "    reg_layer1_val = bn_layer_top(reg_layer1_val, 'reg1', False)\n",
    "\n",
    "reg_layer1_test = tf.matmul(layers_test[-1], reg_weights1) + reg_biases1\n",
    "if 'ReLU' in model_type:\n",
    "    reg_layer1_test = tf.nn.relu(reg_layer1_test)\n",
    "if 'BN2' in model_type:\n",
    "    reg_layer1_test = bn_layer_top(reg_layer1_test, 'reg1', False)\n",
    "\n",
    "\n",
    "######### Output layer of regressor\n",
    "reg_weights = tf.Variable(tf.truncated_normal([128, 1], stddev=0.00005, seed=202002241+seed_difference), name='weights_reg')\n",
    "reg_biases = tf.Variable(tf.truncated_normal([1], mean=0.0, stddev=0.00005, seed=202002241+seed_difference), name='biases_reg')\n",
    "\n",
    "reg_layer = tf.matmul(reg_layer1, reg_weights) + reg_biases\n",
    "reg_layer_val = tf.matmul(reg_layer1_val, reg_weights) + reg_biases\n",
    "reg_layer_test = tf.matmul(reg_layer1_test, reg_weights) + reg_biases\n",
    "\n",
    "reg_Y = tf.placeholder(tf.float32, shape=(trainBatchSize, 1), name='Y_reg')\n",
    "reg_Y_val = tf.placeholder(tf.float32, shape=(valBatchSize, 1), name='Y_reg_reg')\n",
    "\n",
    "def regularizer(W):\n",
    "    return lamb * tf.reduce_mean(tf.square(W))\n",
    "\n",
    "reg_cost = tf.reduce_mean(tf.square(tf.subtract(reg_layer, reg_Y))) # + regularizer(weights['fc1']) + regularizer(reg_weights1) + regularizer(reg_weights)\n",
    "reg_cost_val = tf.reduce_mean(tf.square(tf.subtract(reg_layer_val, reg_Y_val)))\n",
    "\n",
    "    \n",
    "reg_var = [reg_weights1, reg_biases1, reg_weights, reg_biases]\n",
    "reg_var.extend([weights['fc1'], biases['fc1']]) # uncomment for unfreeze layer 1\n",
    "\n",
    "if 'BN2' in model_type:\n",
    "    with tf.variable_scope(\"reg1\", reuse=True):\n",
    "        reg1_beta = tf.get_variable(\"beta\")\n",
    "        reg1_gamma = tf.get_variable(\"gamma\")\n",
    "    reg_var.extend([reg1_beta, reg1_gamma])\n",
    "\n",
    "learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "reg_train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(reg_cost, var_list=reg_var)\n",
    "\n",
    "sess = tf.Session()\n",
    "    \n",
    "var1 = [weights['fc1'], biases['fc1'], weights['fc2_cla'], biases['fc2_cla']]\n",
    "saver = tf.train.Saver(var1)\n",
    "tf.set_random_seed(20200224+seed_difference)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "saver.restore(sess, 'saved_models/1024_530_current'); model_type += ', TL w/o freezing' if weights['fc1'] in reg_var else ', TL'\n",
    "\n",
    "if DA_flag:\n",
    "    model_type += ', DA'\n",
    "    \n",
    "if 'TL' not in model_type and weights['fc1'] not in reg_var:\n",
    "    model_type += ' ** LAYER 1 WRONG SETTING **'\n",
    "    \n",
    "global_trainRecord = []\n",
    "global_valRecord = []\n",
    "global_testRecord = []\n",
    "beginTime = time.time()\n",
    "totalTrainingIters = 0\n",
    "itersInTotal = 0\n",
    "print('Training set size:', len(reg_train_x_data))\n",
    "print('Validation set size:', len(reg_val_x_data))\n",
    "print('Model:', model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "# Initialize training sequence\n",
    "\n",
    "totalTrainingIters += reg_hyper_param['traindatasetSize'] * reg_hyper_param['trainingEpochs'] / reg_hyper_param['trainBatchSize']\n",
    "\n",
    "print('MSE before training: ', reg_validation(sess, reg_val_x_data, reg_val_y_data, reg_hyper_param))\n",
    "\n",
    "train_cost_history = reg_train(sess, reg_train_op, reg_cost, reg_train_x_data, reg_train_y_data, reg_train_delta_data, reg_val_x_data, reg_val_y_data, reg_hyper_param)\n",
    "\n",
    "np.random.seed(812374917+seed_difference)\n",
    "factor = np.random.uniform(low=0.03, high=1.0, size=(reg_hyper_param['traindatasetSize'], 1))\n",
    "reg_train_x_data_for_test = reg_train_x_data + factor * reg_train_delta_data\n",
    "reg_train_y_data_for_test = (1-factor) * reg_train_y_data\n",
    "trainingLoss = reg_validation(sess, reg_train_x_data_for_test, reg_train_y_data_for_test, reg_hyper_param)\n",
    "testingLoss = reg_validation(sess, reg_val_x_data, reg_val_y_data, reg_hyper_param)\n",
    "print(\"train MSE %.2f, validation MSE %.2f\" % (trainingLoss, testingLoss))\n",
    "\n",
    "idx = np.array(global_valRecord)[:,1].argmin()\n",
    "print('train MSE %.6f validation MSE %.6f @ %d epochs'%(np.array(global_trainRecord)[idx,1], np.array(global_valRecord)[idx,1], \n",
    "                                                        np.array(global_valRecord)[idx,0]/(reg_hyper_param['traindatasetSize']/reg_hyper_param['trainBatchSize'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Plot training/validation curves\n",
    "\n",
    "global global_trainRecord, global_valRecord\n",
    "plt.figure(figsize=(18,6))\n",
    "plt.plot(np.array(global_trainRecord)[:,0]/(reg_hyper_param['traindatasetSize']/reg_hyper_param['trainBatchSize']), np.array(global_trainRecord)[:,1], 'r', label=\"Train\")\n",
    "plt.plot(np.array(global_valRecord)[:,0]/(reg_hyper_param['traindatasetSize']/reg_hyper_param['trainBatchSize']), np.array(global_valRecord)[:,1], 'b', label=\"Val\")\n",
    "plt.legend()\n",
    "plt.ylim([0,0.02])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEstimateDistance(sess, data):\n",
    "    dataSize = data.shape[0]\n",
    "    scores = []\n",
    "    for i in range(0, dataSize):\n",
    "        x = [data[i]]\n",
    "        p = sess.run(reg_layer_test, feed_dict={X_test:x})\n",
    "        scores.append(p)\n",
    "    scores = np.array(scores).reshape(dataSize, -1)\n",
    "    return scores\n",
    "\n",
    "\n",
    "# reg_layer_test = tf.matmul(layers_test[-1], reg_weights) + reg_biases\n",
    "t = time.time()\n",
    "s = getEstimateDistance(sess, reg_val_x_data)\n",
    "print(time.time() - t, 's', s.shape, 'samples')\n",
    "\n",
    "%matplotlib inline\n",
    "plt.figure(3, figsize=(6,6))\n",
    "plt.plot(s, reg_val_y_data, '.')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "lim1 = np.min([s, reg_val_y_data])\n",
    "lim2 = np.max([s, reg_val_y_data])\n",
    "# plt.xlim([lim1,lim2])\n",
    "# plt.ylim([lim1,lim2])\n",
    "plt.grid(True)\n",
    "plt.title('Transfer Learning')\n",
    "plt.show()\n",
    "\n",
    "# np.savetxt('scores.csv', s, delimiter=',')\n",
    "from sklearn.metrics import r2_score\n",
    "print('R2 = ', r2_score(s, reg_val_y_data))\n",
    "print('RMSE = ', np.sqrt(np.mean(np.square(s-reg_val_y_data))))\n",
    "print('MSE = ', np.mean(np.square(s-reg_val_y_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
